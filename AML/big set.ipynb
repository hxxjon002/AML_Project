{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbed7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F_torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6432594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = 0.8\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64877617",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt('ml-32m/ratings.csv', delimiter=',', skip_header=1)\n",
    "\n",
    "u_idx = {}\n",
    "i_idx = {}\n",
    "idx_u = []\n",
    "idx_i = []\n",
    "data_by_user_train = []\n",
    "data_by_item_train = []\n",
    "\n",
    "data_by_user_test = []\n",
    "data_by_item_test = []\n",
    "\n",
    "for i in range(data.shape[0]):\n",
    "    user = int(data[i, 0])\n",
    "    item = int(data[i, 1])\n",
    "    rating = data[i, 2]\n",
    "\n",
    "    if user not in u_idx:\n",
    "        u_idx[user] = len(u_idx)\n",
    "        idx_u.append(user)\n",
    "        data_by_user_train.append([])\n",
    "        data_by_user_test.append([])\n",
    "\n",
    "    if item not in i_idx:\n",
    "        i_idx[item] = len(i_idx)\n",
    "        idx_i.append(item)\n",
    "        data_by_item_train.append([])\n",
    "        data_by_item_test.append([])\n",
    "    \n",
    "    u = (i_idx[item],rating)\n",
    "    i = (u_idx[user],rating)\n",
    "\n",
    "    if np.random.rand() < train_test_split:\n",
    "        data_by_user_train[u_idx[user]].append(u)\n",
    "        data_by_item_train[i_idx[item]].append(i)\n",
    "    else:\n",
    "        data_by_user_test[u_idx[user]].append(u)\n",
    "        data_by_item_test[i_idx[item]].append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ad79d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(dataset, x):\n",
    "    all_ratings = []\n",
    "    ptr = [0]\n",
    "    for ratings in dataset:\n",
    "        all_ratings.extend(ratings)\n",
    "        ptr.append(len(all_ratings))\n",
    "   \n",
    "    all_ratings = np.array(all_ratings, dtype=[(x, int), ('rating', float)])\n",
    "    ptr = np.array(ptr, dtype=int)\n",
    "    return all_ratings, ptr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c31e7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_train , user_train_ptr = vectorize(data_by_user_train, x='item')\n",
    "user_test , user_test_ptr = vectorize(data_by_user_test, x='item')\n",
    "\n",
    "item_train, item_train_ptr = vectorize(data_by_item_train, x='user')\n",
    "item_test, item_test_ptr = vectorize(data_by_item_test, x='user')\n",
    "\n",
    "\n",
    "N = len(item_train_ptr) - 1\n",
    "num_ratings = np.zeros(N, dtype=int)\n",
    "for n in range(N):\n",
    "    num_ratings[n] = (item_train_ptr[n+1] - item_train_ptr[n]) + (item_test_ptr[n+1] - item_test_ptr[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db7538e",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_id_to_genres = {}\n",
    "movie_titles = {}\n",
    "all_genres = set()\n",
    "\n",
    "with open(\"ml-32m/movies.csv\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)\n",
    "    for movie_id_str, title, genres_str in reader:\n",
    "        movie_id = int(movie_id_str)\n",
    "        if genres_str == \"(no genres listed)\" :\n",
    "            genres = []  \n",
    "        else:\n",
    "            genres = genres_str.split(\"|\")\n",
    "\n",
    "        movie_id_to_genres[movie_id] = genres\n",
    "    \n",
    "        movie_titles[movie_id] = title  \n",
    "        all_genres.update(genres)\n",
    "\n",
    "all_genres = sorted(all_genres)\n",
    "genre_to_index = {g: i for i, g in enumerate(all_genres)}\n",
    "num_features = len(all_genres)\n",
    "\n",
    "N = len(idx_i)   \n",
    "item_features = np.zeros((N, num_features), dtype=np.int8)\n",
    "\n",
    "for n, movie_id in enumerate(idx_i):\n",
    "    genres = movie_id_to_genres.get(movie_id, [])\n",
    "    for g in genres:\n",
    "        if g in genre_to_index:\n",
    "            item_features[n, genre_to_index[g]] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688b7807",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_train = user_train['rating']\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(ratings_train, bins=np.arange(min(ratings_train), max(ratings_train) + 0.5, 0.5), edgecolor='black')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of user ratings (Train Set)')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.savefig(\"Rating distribution histogram.pdf\", format=\"pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe79b03",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "u_degrees = user_train_ptr[1:] - user_train_ptr[:-1] \n",
    "i_degrees = item_train_ptr[1:] - item_train_ptr[:-1]  \n",
    "\n",
    "u_freq = Counter(u_degrees)\n",
    "i_freq = Counter(i_degrees)\n",
    "\n",
    "u_deg, u_counts = zip(*sorted(u_freq.items()))\n",
    "i_deg, i_counts = zip(*sorted(i_freq.items()))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.loglog(i_deg, i_counts, 'go', label='items', alpha=0.6)\n",
    "plt.loglog(u_deg, u_counts, 'bs', label='users', alpha=0.6)\n",
    "\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('User and item degree distributions')\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", ls=\"--\", lw=0.5)\n",
    "plt.savefig(\"Power law distribution.pdf\", format=\"pdf\") \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94825c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "K=10\n",
    "lam = 0.05\n",
    "tau = 0.1\n",
    "gamma = 0.1\n",
    "\n",
    "M = len(u_idx)\n",
    "N = len(i_idx)\n",
    "max_iters = 50\n",
    "tol = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21364172",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bias only\n",
    "def ALS_Bias(user_train, item_train, user_test, item_test, user_train_ptr, item_train_ptr, user_test_ptr, item_test_ptr, M, N, lam=1, gamma=0.1,max_iters=100, tol=1e-4, log=True):\n",
    "    train_loss_history = []\n",
    "    test_loss_history = []\n",
    "    train_RMSE_history = []\n",
    "    test_RMSE_history = []\n",
    " \n",
    "\n",
    "    user_biases = np.zeros((M))\n",
    "    item_biases = np.zeros((N))\n",
    "\n",
    "    for iteration in range(max_iters):\n",
    "        for m in range(M):\n",
    "            start = user_train_ptr[m]\n",
    "            end = user_train_ptr[m+1]\n",
    "            user_slice = user_train[start:end]\n",
    "            item_ids = user_slice['item']\n",
    "            ratings = user_slice['rating']\n",
    "\n",
    "            if len(item_ids) == 0:\n",
    "                continue\n",
    "            \n",
    "            i_bias = item_biases[item_ids]\n",
    "\n",
    "            bias_residuals = ratings - i_bias\n",
    "\n",
    "            denom = lam * len(item_ids) + gamma\n",
    "\n",
    "            if denom == 0:\n",
    "                user_biases[m] = 0\n",
    "            else:\n",
    "                user_biases[m] = lam * np.sum(bias_residuals) / denom\n",
    "\n",
    "\n",
    "        for n in range(N):\n",
    "            start = item_train_ptr[n]\n",
    "            end = item_train_ptr[n+1]\n",
    "            item_slice = item_train[start:end]\n",
    "            user_ids = item_slice['user']\n",
    "            ratings = item_slice['rating']\n",
    "\n",
    "            if len(user_ids) == 0:\n",
    "                continue\n",
    "\n",
    "            u_bias = user_biases[user_ids]  \n",
    "            bias_residuals = ratings - u_bias\n",
    "\n",
    "            denom = lam * len(user_ids) + gamma\n",
    "            if denom == 0:\n",
    "                item_biases[n] = 0\n",
    "            else:\n",
    "                item_biases[n] = lam * np.sum(bias_residuals) / denom\n",
    "\n",
    "\n",
    "        train_err =  0\n",
    "        train_count = 0\n",
    "        \n",
    "        test_err = 0\n",
    "        test_count = 0\n",
    "\n",
    "        for m in range(M):\n",
    "            u_bias = user_biases[m]\n",
    "\n",
    "            start = user_train_ptr[m]\n",
    "            end = user_train_ptr[m+1]\n",
    "            user_slice = user_train[start:end]\n",
    "\n",
    "            if len(user_slice) > 0:\n",
    "                item_ids = user_slice['item']\n",
    "                ratings = user_slice['rating']\n",
    "                preds = u_bias + item_biases[item_ids]\n",
    "                train_err += np.sum((ratings - preds) ** 2)\n",
    "                train_count += len(ratings)\n",
    "        \n",
    "            start = user_test_ptr[m]\n",
    "            end = user_test_ptr[m+1]\n",
    "            user_slice = user_test[start:end]\n",
    "            if len(user_slice) > 0:\n",
    "                item_ids = user_slice['item']\n",
    "                ratings = user_slice['rating']\n",
    "                preds = u_bias + item_biases[item_ids]\n",
    "                test_err += np.sum((ratings - preds) ** 2)\n",
    "                test_count += len(ratings)\n",
    "\n",
    "        \n",
    "        train_loss = 0.5 * lam  * train_err + 0.5 * gamma * (np.sum(user_biases ** 2) + np.sum(item_biases ** 2)) \n",
    "        train_loss_history.append(train_loss)\n",
    "\n",
    "        train_RMSE = np.sqrt(train_err / train_count)\n",
    "        train_RMSE_history.append(train_RMSE)\n",
    "\n",
    "        test_RMSE = np.sqrt(test_err / test_count)\n",
    "        test_RMSE_history.append(test_RMSE)\n",
    "\n",
    "        test_loss = 0.5 * lam  * test_err + 0.5 * gamma * (np.sum(user_biases ** 2) + np.sum(item_biases ** 2)) \n",
    "        test_loss_history.append(test_loss)\n",
    "\n",
    "        if log:\n",
    "            print(f\"Iter {iteration+1:3d} | \" f\"Train RMSE: {train_RMSE:.4f} | \" f\"Test RMSE: {test_RMSE:.4f} | \" f\"Train Loss: {train_loss:.2f} | \" f\"Test Loss: {test_loss:.2f}\")\n",
    "\n",
    "        if iteration > 4 and abs(test_RMSE_history[-1] - test_RMSE_history[-5]) < tol:\n",
    "            if log:\n",
    "                print(f\"Early stopping at iteration {iteration + 1}\")\n",
    "            break\n",
    "\n",
    "    history = {\"train_loss\": train_loss_history,\"test_loss\": test_loss_history, \"train_RMSE\": train_RMSE_history, \"test_RMSE\": test_RMSE_history}\n",
    "\n",
    "    return user_biases, item_biases, history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e507f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_biases, i_biases, history =   ALS_Bias(user_train, item_train, user_test, item_test, user_train_ptr, item_train_ptr, \n",
    "                                user_test_ptr, item_test_ptr, M, N, lam=lam, gamma=gamma, max_iters=max_iters, tol=tol, log=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b69689b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(history[\"train_loss\"], marker='o')\n",
    "plt.title(\"Loss over Iterations\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"ALS TRAIN LOSS BIAS.pdf\", format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214a1d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(history[\"test_loss\"], marker='o')\n",
    "plt.title(\"Loss over Iterations\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"ALS TEST LOSS BIAS.pdf\", format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ec38b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(history[\"train_RMSE\"], marker='o')\n",
    "plt.plot(history[\"test_RMSE\"], marker='*')\n",
    "plt.legend(['Train Loss', 'Test Loss'])\n",
    "plt.title(\"RMSE over Iterations - ALS with bias\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"ALS RMSE BIAS.pdf\", format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6bf85e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def ALS_Embedding(user_train, item_train, user_test, item_test, user_train_ptr, item_train_ptr, user_test_ptr, item_test_ptr, M, N,K,lam=1, gamma=0.1, tau=1, max_iters=100, tol=1e-4, log=True):\n",
    "    train_loss_history = []\n",
    "    test_loss_history = []\n",
    "    train_RMSE_history = []\n",
    "    test_RMSE_history = []\n",
    "\n",
    "    user_biases = np.zeros((M))\n",
    "    item_biases = np.zeros((N))\n",
    "\n",
    "    U = np.random.normal(0, 0.1, size=(M, K))\n",
    "    V = np.random.normal(0, 0.1, size=(N, K))\n",
    "\n",
    "    for iteration in range(max_iters):\n",
    "        for m in range(M):\n",
    "            start = user_train_ptr[m]\n",
    "            end = user_train_ptr[m+1]\n",
    "            user_slice = user_train[start:end]\n",
    "            item_ids = user_slice['item']\n",
    "            ratings = user_slice['rating']\n",
    "\n",
    "            if len(item_ids) == 0:\n",
    "                continue\n",
    "            \n",
    "            v = V[item_ids]\n",
    "            i_bias = item_biases[item_ids]\n",
    "\n",
    "            bias_residuals = ratings - np.dot(U[m], v.T) - i_bias\n",
    "\n",
    "            denom = lam * len(item_ids) + gamma\n",
    "            if denom == 0:\n",
    "                user_biases[m] = 0\n",
    "            else:\n",
    "                user_biases[m] = lam * np.sum(bias_residuals) / denom\n",
    "\n",
    "            vector_residual = ratings - i_bias - user_biases[m]\n",
    "            residual = lam * np.sum(vector_residual[:, np.newaxis] * v, axis=0)\n",
    "            A = lam * np.dot(v.T, v) + tau * np.eye(K)\n",
    "            \n",
    "            U[m] = np.linalg.solve(A, residual)\n",
    "\n",
    "\n",
    "        for n in range(N):\n",
    "            start = item_train_ptr[n]\n",
    "            end = item_train_ptr[n+1]\n",
    "            item_slice = item_train[start:end]\n",
    "            user_ids = item_slice['user']\n",
    "            ratings = item_slice['rating']\n",
    "\n",
    "            if len(user_ids) == 0:\n",
    "                continue\n",
    "\n",
    "            u = U[user_ids]\n",
    "            u_bias = user_biases[user_ids]\n",
    "\n",
    "            bias_residuals = ratings - np.dot(u, V[n]) - u_bias\n",
    "\n",
    "            denom = lam * len(user_ids) + gamma\n",
    "            if denom == 0:\n",
    "                item_biases[n] = 0\n",
    "            else:\n",
    "                item_biases[n] = lam * np.sum(bias_residuals) / denom\n",
    "\n",
    "            vector_residual = ratings - u_bias - item_biases[n]\n",
    "            residual = lam * np.sum(vector_residual[:, np.newaxis] * u, axis=0)\n",
    "            A = lam * np.dot(u.T, u) + tau * np.eye(K)\n",
    "\n",
    "            V[n] = np.linalg.solve(A, residual)\n",
    "\n",
    "\n",
    "        train_err =  0\n",
    "        train_count = 0\n",
    "        \n",
    "        test_err = 0\n",
    "        test_count = 0\n",
    "\n",
    "        for m in range(M):\n",
    "            u_bias = user_biases[m]\n",
    "            u_vec = U[m]\n",
    "\n",
    "            start = user_train_ptr[m]\n",
    "            end = user_train_ptr[m+1]\n",
    "            user_slice = user_train[start:end]\n",
    "            if len(user_slice) > 0:\n",
    "                item_ids = user_slice['item']\n",
    "                ratings = user_slice['rating']\n",
    "                preds = np.dot(u_vec, V[item_ids].T) + u_bias + item_biases[item_ids]\n",
    "                train_err += np.sum((ratings - preds) ** 2)\n",
    "                train_count += len(ratings)\n",
    "        \n",
    "            start = user_test_ptr[m]\n",
    "            end = user_test_ptr[m+1]\n",
    "            user_slice = user_test[start:end]\n",
    "            if len(user_slice) > 0:\n",
    "                item_ids = user_slice['item']\n",
    "                ratings = user_slice['rating']\n",
    "                preds = np.dot(u_vec, V[item_ids].T) + u_bias + item_biases[item_ids]\n",
    "                test_err += np.sum((ratings - preds) ** 2)\n",
    "                test_count += len(ratings)\n",
    "\n",
    "        \n",
    "        train_loss = 0.5 * lam  * train_err + 0.5 * gamma * (np.sum(user_biases ** 2) + np.sum(item_biases ** 2)) + 0.5 * tau * (np.sum(U**2) + np.sum(V**2)) \n",
    "        train_loss_history.append(train_loss)\n",
    "\n",
    "        train_RMSE = np.sqrt(train_err / train_count)\n",
    "        train_RMSE_history.append(train_RMSE)\n",
    "\n",
    "        test_RMSE = np.sqrt(test_err / test_count)\n",
    "        test_RMSE_history.append(test_RMSE)\n",
    "\n",
    "        test_loss = 0.5 * lam  * test_err + 0.5 * gamma * (np.sum(user_biases ** 2) + np.sum(item_biases ** 2)) + 0.5 * tau * (np.sum(U**2) + np.sum(V**2)) \n",
    "        test_loss_history.append(test_loss)\n",
    "\n",
    "        if log:\n",
    "            print(f\"Iter {iteration+1:3d} | \" f\"Train RMSE: {train_RMSE:.4f} | \" f\"Test RMSE: {test_RMSE:.4f} | \" f\"Train Loss: {train_loss:.2f} | \" f\"Test Loss: {test_loss:.2f}\")\n",
    "\n",
    "        if iteration > 4 and (abs(test_RMSE_history[-1] - test_RMSE_history[-5]) < tol or test_RMSE_history[-1] > test_RMSE_history[-2]):\n",
    "            if log:\n",
    "                print(f\"Early stopping at iteration {iteration + 1}\")\n",
    "            break\n",
    "    \n",
    "    history = {\"train_loss\": train_loss_history,\"test_loss\": test_loss_history, \"train_RMSE\": train_RMSE_history, \"test_RMSE\": test_RMSE_history}\n",
    "\n",
    "    return user_biases, item_biases, U, V ,history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b5eaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_biases, i_biases, U , V, history =  ALS_Embedding(user_train, item_train, user_test, item_test, user_train_ptr, item_train_ptr, \n",
    "                                user_test_ptr, item_test_ptr, M, N, K, lam=lam, gamma=gamma, tau = tau, max_iters=max_iters, tol=tol, log=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb7bace",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(history[\"train_loss\"], marker='o')\n",
    "plt.title(\"Train loss over Iterations\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"ALS TRAIN LOSS EMBEDDING.pdf\", format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2f8cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(history[\"test_loss\"], marker='o')\n",
    "plt.title(\"Test loss over Iterations\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"ALS TEST LOSS EMBEDDING.pdf\", format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5a653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(history[\"train_RMSE\"], marker='o')\n",
    "plt.plot(history[\"test_RMSE\"], marker='*')\n",
    "plt.legend(['Train Loss', 'Test Loss'])\n",
    "plt.title(\"RMSE over Iterations - ALS with bias and embedding\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"ALS RMSE EMBEDDING.pdf\", format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145f96ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "V_2d = pca.fit_transform(V)\n",
    "\n",
    "Horror = [39446, 48877, 55577]\n",
    "SciFi = [260, 1196, 1210]\n",
    "RomCom = [3, 7, 64]\n",
    "Children = [78499, 49274, 62999]\n",
    "\n",
    "groups = { \"Horror\": Horror, \"Sci-Fi\": SciFi, \"RomCom\": RomCom, \"Children\": Children}\n",
    "colors = {\"Horror\": \"tab:orange\", \"Sci-Fi\": \"tab:blue\", \"RomCom\": \"tab:red\",\"Children\": \"tab:purple\"}\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(V_2d[:, 0], V_2d[:, 1], s=3, c=\"#00cc44\", alpha=0.2, label=\"All movies\")\n",
    "\n",
    "for group_name, movie_ids in groups.items():\n",
    "    indices = [idx_i.index(mid) for mid in movie_ids if mid in idx_i]\n",
    "    plt.scatter(V_2d[indices, 0], V_2d[indices, 1], s=50, c=colors[group_name], label=group_name,\n",
    "                edgecolors=\"black\", linewidths=0.5)\n",
    "    for idx in indices:\n",
    "        movie_id = idx_i[idx]\n",
    "        title = movie_titles.get(movie_id, f\"movieId={movie_id}\")\n",
    "        plt.text(V_2d[idx, 0] + 0.02, V_2d[idx, 1] + 0.02, title, fontsize=8)\n",
    "\n",
    "\n",
    "plt.axhline(0, lw=0.8, color=\"gray\", alpha=0.5)\n",
    "plt.axvline(0, lw=0.8, color=\"gray\", alpha=0.5)\n",
    "plt.title(\"2D Embedding of Movies\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"2D embeddings.pdf\", format=\"pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4c11dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorized with features\n",
    "def ALS_Features(user_train, item_train, user_test, item_test, user_train_ptr, item_train_ptr, user_test_ptr, item_test_ptr, M, N,K, item_features , lam=1, gamma=0.1, tau=1, max_iters=100, tol=1e-4, log=True):\n",
    "    train_loss_history = []\n",
    "    test_loss_history = []\n",
    "    train_RMSE_history = []\n",
    "    test_RMSE_history = []\n",
    "\n",
    "    user_biases = np.zeros((M))\n",
    "    item_biases = np.zeros((N))\n",
    "    num_features = item_features.shape[1]\n",
    "\n",
    "    U = np.random.normal(0, 0.1, size=(M, K))\n",
    "    V = np.random.normal(0, 0.1, size=(N, K))\n",
    "    F = np.random.normal(0, 0.1, size=(num_features, K))\n",
    "\n",
    "    for iteration in range(max_iters):\n",
    "        for m in range(M):\n",
    "            start = user_train_ptr[m]\n",
    "            end = user_train_ptr[m+1]\n",
    "            user_slice = user_train[start:end]\n",
    "            item_ids = user_slice['item']\n",
    "            ratings = user_slice['rating']\n",
    "\n",
    "            if len(item_ids) == 0:\n",
    "                continue\n",
    "            \n",
    "            v = V[item_ids]\n",
    "            i_bias = item_biases[item_ids]\n",
    "\n",
    "            bias_residuals = ratings - np.dot(U[m], v.T) - i_bias\n",
    "\n",
    "            denom = lam * len(item_ids) + gamma\n",
    "            if denom == 0:\n",
    "                user_biases[m] = 0\n",
    "            else :\n",
    "                user_biases[m] = lam * np.sum(bias_residuals) / denom\n",
    "\n",
    "            vector_residual = ratings - i_bias - user_biases[m]\n",
    "            residual = lam * np.sum(vector_residual[:, np.newaxis] * v, axis=0)\n",
    "            A = lam * np.dot(v.T, v) + tau * np.eye(K)\n",
    "            U[m] = np.linalg.solve(A, residual)\n",
    "\n",
    "        for n in range(N):\n",
    "            start = item_train_ptr[n]\n",
    "            end = item_train_ptr[n+1]\n",
    "            item_slice = item_train[start:end]\n",
    "            user_ids = item_slice['user']\n",
    "            ratings = item_slice['rating']\n",
    "\n",
    "            if len(user_ids) == 0:\n",
    "                continue\n",
    "\n",
    "            u = U[user_ids]\n",
    "            u_bias = user_biases[user_ids]\n",
    "\n",
    "            bias_residuals = ratings - np.dot(u, V[n]) - u_bias\n",
    "            denom = lam * len(user_ids) + gamma\n",
    "            if denom == 0:\n",
    "                item_biases[n] = 0 \n",
    "            else:\n",
    "                item_biases[n] = lam * np.sum(bias_residuals) / denom\n",
    "\n",
    "            vector_residual = ratings - u_bias - item_biases[n]\n",
    "            residual = lam * np.sum(vector_residual[:, np.newaxis] * u, axis=0)\n",
    "            A = lam * np.dot(u.T, u) + tau * np.eye(K)\n",
    "\n",
    "            f_idx = np.flatnonzero(item_features[n])  \n",
    "            Fn = len(f_idx)\n",
    "            if Fn > 0:\n",
    "                feature_mean = F[f_idx].sum(axis=0) / np.sqrt(Fn)\n",
    "            else:\n",
    "                feature_mean = np.zeros(K)\n",
    "            V[n] = np.linalg.solve(A, residual + tau * feature_mean)\n",
    "\n",
    "        for l in range(num_features):\n",
    "            items_with_l = np.flatnonzero(item_features[:, l])\n",
    "            if items_with_l.size == 0:\n",
    "                continue\n",
    "\n",
    "            num = np.zeros(K)\n",
    "            denom = 1\n",
    "            for n in items_with_l:\n",
    "                f_idx = np.flatnonzero(item_features[n])\n",
    "                Fn = len(f_idx)\n",
    "                if Fn == 0:\n",
    "                    continue\n",
    "\n",
    "                sum_except_l = F[f_idx].sum(axis=0) - F[l]\n",
    "                num += (V[n] - (sum_except_l / np.sqrt(Fn))) / np.sqrt(Fn)   \n",
    "                denom += 1 / Fn\n",
    "            F[l] = num / denom\n",
    "\n",
    "\n",
    "        train_err =  0\n",
    "        train_count = 0\n",
    "        test_err = 0\n",
    "        test_count = 0\n",
    "\n",
    "        for m in range(M):\n",
    "            u_bias = user_biases[m]\n",
    "            u_vec = U[m]\n",
    "            start = user_train_ptr[m]\n",
    "            end = user_train_ptr[m+1]\n",
    "            user_slice = user_train[start:end]\n",
    "            if len(user_slice) > 0:\n",
    "                item_ids = user_slice['item']\n",
    "                ratings = user_slice['rating']\n",
    "                preds = np.dot(u_vec, V[item_ids].T) + u_bias + item_biases[item_ids]\n",
    "                train_err += np.sum((ratings - preds) ** 2)\n",
    "                train_count += len(ratings)\n",
    "            start = user_test_ptr[m]\n",
    "            end = user_test_ptr[m+1]\n",
    "            user_slice = user_test[start:end]\n",
    "            if len(user_slice) > 0:\n",
    "                item_ids = user_slice['item']\n",
    "                ratings = user_slice['rating']\n",
    "                preds = np.dot(u_vec, V[item_ids].T) + u_bias + item_biases[item_ids]\n",
    "                test_err += np.sum((ratings - preds) ** 2)\n",
    "                test_count += len(ratings)\n",
    "        \n",
    "        feature_prior = 0\n",
    "        for n in range(N):\n",
    "            f_idx = np.flatnonzero(item_features[n])\n",
    "            Fn = len(f_idx)\n",
    "            if Fn > 0:\n",
    "                feature_mean = F[f_idx].sum(axis=0) / np.sqrt(Fn)\n",
    "            else:\n",
    "                feature_mean = np.zeros(K)\n",
    "            feature_prior += np.sum((V[n] - feature_mean) ** 2)\n",
    "\n",
    "        train_loss = 0.5 * lam  * train_err + 0.5 * gamma * (np.sum(user_biases ** 2) + np.sum(item_biases ** 2)) + 0.5 * tau * (np.sum(U**2) +feature_prior + np.sum(F**2)) \n",
    "        train_loss_history.append(train_loss)\n",
    "\n",
    "        train_RMSE = np.sqrt(train_err / train_count)\n",
    "        train_RMSE_history.append(train_RMSE)\n",
    "\n",
    "        test_RMSE = np.sqrt(test_err / test_count)\n",
    "        test_RMSE_history.append(test_RMSE)\n",
    "\n",
    "        test_loss = 0.5 * lam  * test_err + 0.5 * gamma * (np.sum(user_biases ** 2) + np.sum(item_biases ** 2)) + 0.5 * tau * (np.sum(U**2) +feature_prior + np.sum(F**2)) \n",
    "        test_loss_history.append(test_loss)\n",
    "\n",
    "        if log:\n",
    "            print(f\"Iter {iteration+1:3d} | \" f\"Train RMSE: {train_RMSE:.4f} | \" f\"Test RMSE: {test_RMSE:.4f} | \" f\"Train Loss: {train_loss:.2f} | \" f\"Test Loss: {test_loss:.2f}\")\n",
    "\n",
    "        if iteration > 4 and abs(test_RMSE_history[-1] - test_RMSE_history[-5]) < tol:\n",
    "            if log:\n",
    "                print(f\"Early stopping at iteration {iteration + 1}\")\n",
    "            break\n",
    "\n",
    "    history = {\"train_loss\": train_loss_history,\"test_loss\": test_loss_history, \"train_RMSE\": train_RMSE_history, \"test_RMSE\": test_RMSE_history}\n",
    "\n",
    "    return user_biases, item_biases, U, V , F, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34086f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_biases, i_biases, U , V, F, history =  ALS_Features(user_train, item_train, user_test, item_test, user_train_ptr, item_train_ptr, \n",
    "                                user_test_ptr, item_test_ptr, M, N, K, item_features,lam=lam, gamma=gamma, tau = tau, max_iters=max_iters, tol=tol, log=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03c9d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(history[\"train_loss\"], marker='o')\n",
    "plt.title(\"Loss over Iterations\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"ALS TRAIN LOSS FEATURES.pdf\", format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5579a1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(history[\"test_loss\"], marker='o')\n",
    "plt.title(\"Loss over Iterations\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"ALS TEST LOSS FEATURES.pdf\", format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c348e1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(history[\"train_RMSE\"], marker='o')\n",
    "plt.plot(history[\"test_RMSE\"], marker='*')\n",
    "plt.legend(['Train Loss', 'Test Loss'])\n",
    "plt.title(\"RMSE over Iterations - ALS with bias, embedding and features\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"ALS RMSE FEATURES.pdf\", format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27810d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "V_2d = pca.fit_transform(V)\n",
    "\n",
    "Horror = [39446, 48877, 55577]\n",
    "SciFi = [260, 1196, 1210]\n",
    "RomCom = [3, 7, 64]\n",
    "Children = [78499, 49274, 62999]\n",
    "\n",
    "groups = { \"Horror\": Horror, \"Sci-Fi\": SciFi, \"RomCom\": RomCom, \"Children\": Children}\n",
    "colors = {\"Horror\": \"tab:orange\", \"Sci-Fi\": \"tab:blue\", \"RomCom\": \"tab:red\",\"Children\": \"tab:purple\"}\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(V_2d[:, 0], V_2d[:, 1], s=3, c=\"#00cc44\", alpha=0.2, label=\"All movies\")\n",
    "\n",
    "for group_name, movie_ids in groups.items():\n",
    "    indices = [idx_i.index(mid) for mid in movie_ids if mid in idx_i]\n",
    "    plt.scatter(V_2d[indices, 0], V_2d[indices, 1], s=50, c=colors[group_name], label=group_name,\n",
    "                edgecolors=\"black\", linewidths=0.5)\n",
    "    for idx in indices:\n",
    "        movie_id = idx_i[idx]\n",
    "        title = movie_titles.get(movie_id, f\"movieId={movie_id}\")\n",
    "        plt.text(V_2d[idx, 0] + 0.02, V_2d[idx, 1] + 0.02, title, fontsize=8)\n",
    "\n",
    "\n",
    "plt.axhline(0, lw=0.8, color=\"gray\", alpha=0.5)\n",
    "plt.axvline(0, lw=0.8, color=\"gray\", alpha=0.5)\n",
    "plt.title(\"2D Embedding of Movies\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"2D embeddings with features.pdf\", format=\"pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b02df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, random_state=0)\n",
    "V_2d = pca.fit_transform(V)         \n",
    "F_2d = pca.transform(F)              \n",
    "\n",
    "index_to_genre = {i: g for g, i in genre_to_index.items()}\n",
    "\n",
    "genres_to_annotate = [\"Action\", \"Comedy\", \"Drama\", \"Sci-Fi\", \"Romance\"]\n",
    "annot_idx = [genre_to_index[g] for g in genres_to_annotate if g in genre_to_index]\n",
    "\n",
    "plt.figure(figsize=(11, 8.5))\n",
    "\n",
    "plt.scatter(V_2d[:, 0], V_2d[:, 1], s=3, c=\"#00cc44\", alpha=0.18, label=\"All movies\")\n",
    "\n",
    "plt.scatter(F_2d[:, 0], F_2d[:, 1], s=120, marker=\"*\", edgecolors=\"black\",\n",
    "            linewidths=0.6, alpha=0.95, c=\"tab:purple\", label=\"Feature vectors (genres)\")\n",
    "\n",
    "for gi in range(F_2d.shape[0]):\n",
    "    x, y = F_2d[gi]\n",
    "    plt.text(x + 0.02, y + 0.02, index_to_genre[gi], fontsize=8)\n",
    "\n",
    "\n",
    "plt.axhline(0, lw=0.8, color=\"gray\", alpha=0.45)\n",
    "plt.axvline(0, lw=0.8, color=\"gray\", alpha=0.45)\n",
    "plt.title(\"2D PCA of Movie Embeddings with Features\", fontsize=12)\n",
    "plt.legend(loc=\"best\", fontsize=9, frameon=True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"FEATURE EMBEDDINGS.pdf\", format=\"pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3badb45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cars 45517  LOTR 2116, 4993   Cold start movie 52696\n",
    "\n",
    "#Horror = [39446, 48877, 55577]\n",
    "#SciFi = [260, 1196, 1210]\n",
    "#RomCom = [3, 7, 64]\n",
    "#Superhero = [33794, 59315, 77561]\n",
    "\n",
    "movieID = 79363\n",
    "\n",
    "r_dummy = 5\n",
    "u_dummy = np.zeros(K)\n",
    "b_dummy = 0\n",
    "\n",
    "is_cold_start = False\n",
    "\n",
    "if movieID in i_idx:\n",
    "    item_idx = i_idx[movieID]\n",
    "    v = V[item_idx]\n",
    "    i_bias = i_biases[item_idx]\n",
    "else:\n",
    "    is_cold_start = True\n",
    "    genres = movie_id_to_genres.get(movieID, [])\n",
    "    f_idx = [genre_to_index[g] for g in genres if g in genre_to_index]\n",
    "\n",
    "    if not f_idx:\n",
    "        v = np.random.normal(0, 0.01, size=K)\n",
    "        i_bias = 0\n",
    "    else:\n",
    "        Fn = len(f_idx)\n",
    "        v = F[f_idx].sum(axis=0) / np.sqrt(Fn)  \n",
    "        i_bias = np.mean(i_biases)                    \n",
    "\n",
    "for i in range(20):\n",
    "    b_dummy = lam * (r_dummy - np.dot(u_dummy, v) - i_bias) / (lam + gamma)\n",
    "    u_dummy = ((r_dummy - b_dummy - i_bias) * v) / (np.dot(v, v) + tau)\n",
    "\n",
    "scores = V @ u_dummy + 0.05 * i_biases\n",
    "\n",
    "min_ratings = 100\n",
    "scores = np.where(np.array(num_ratings) >= min_ratings, scores, -np.inf)\n",
    "\n",
    "top_items = np.argsort(scores)[::-1]\n",
    "top_n = 5\n",
    "shown = 0\n",
    "\n",
    "print(f\"\\nTop {top_n} recommendations for a dummy \" f\"'{movie_titles.get(movieID, f'movieId={movieID}')}' fan \")\n",
    "\n",
    "for idx in top_items:\n",
    "    movie_id = idx_i[idx]\n",
    "    if movie_id == movieID:\n",
    "        continue\n",
    "    title = movie_titles.get(movie_id, f\"movieId={movie_id}\")\n",
    "    genres = movie_id_to_genres.get(movie_id, [\"(no genres listed)\"])\n",
    "\n",
    "    print(f\"{shown+1:2d}. {title} â€” Genres: {', '.join(genres)}\")\n",
    "    shown += 1\n",
    "    if shown >= top_n:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02304621",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_norms = np.linalg.norm(V, axis=1)\n",
    "\n",
    "min_ratings = 100\n",
    "valid_indices = [i for i, n in enumerate(num_ratings) if n >= min_ratings]\n",
    "\n",
    "polarizing_indices = sorted(valid_indices, key=lambda i: movie_norms[i], reverse=True)\n",
    "least_polarizing_indices = sorted(valid_indices, key=lambda i: movie_norms[i])\n",
    "\n",
    "top_n = 10\n",
    "\n",
    "print(f\"\\nTop {top_n} MOST polarizing movies:\")\n",
    "for idx in polarizing_indices[:top_n]:\n",
    "    movie_id = idx_i[idx]\n",
    "    print(f\"- {movie_titles.get(movie_id, f'movieId={movie_id}')} \" f\"(norm={movie_norms[idx]:.4f}, ratings={num_ratings[idx]})\")\n",
    "\n",
    "print(f\"\\nTop {top_n} LEAST polarizing movies:\")\n",
    "for idx in least_polarizing_indices[:top_n]:\n",
    "    movie_id = idx_i[idx]\n",
    "    print(f\"- {movie_titles.get(movie_id, f'movieId={movie_id}')} \" f\"(norm={movie_norms[idx]:.4f}, ratings={num_ratings[idx]})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3121194a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt('ml-32m/ratings.csv', delimiter=',', skip_header=1)\n",
    "\n",
    "u_idx = {}\n",
    "i_idx = {}\n",
    "idx_u = []\n",
    "idx_i = []\n",
    "\n",
    "data_by_user = []\n",
    "data_by_item = []\n",
    "\n",
    "for i in range(data.shape[0]):\n",
    "    user = int(data[i, 0])\n",
    "    item = int(data[i, 1])\n",
    "    rating = data[i, 2]\n",
    "\n",
    "    if user not in u_idx:\n",
    "        u_idx[user] = len(u_idx)\n",
    "        idx_u.append(user)\n",
    "        data_by_user.append([])\n",
    "\n",
    "    if item not in i_idx:\n",
    "        i_idx[item] = len(i_idx)\n",
    "        idx_i.append(item)\n",
    "        data_by_item.append([])\n",
    "    \n",
    "    u = (i_idx[item],rating)\n",
    "    i = (u_idx[user],rating)\n",
    "\n",
    " \n",
    "    data_by_user[u_idx[user]].append(u)\n",
    "    data_by_item[i_idx[item]].append(i)\n",
    "\n",
    "\n",
    "user_full , user_full_ptr = vectorize(data_by_user, x='item')\n",
    "item_full, item_full_ptr = vectorize(data_by_item, x='user')\n",
    "\n",
    "N = len(item_full) - 1\n",
    "num_ratings = np.diff(item_full_ptr) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b81f5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorized with features full set\n",
    "def ALS_Features_Full(user_full, item_full, user_full_ptr, item_full_ptr, M, N,K, item_features , lam=1, gamma=0.1, tau=1, max_iters=100, tol=1e-4, log=True):\n",
    "    loss_history = []\n",
    "    RMSE_history = []\n",
    "\n",
    "    user_biases = np.zeros((M))\n",
    "    item_biases = np.zeros((N))\n",
    "    num_features = item_features.shape[1]\n",
    "\n",
    "    U = np.random.normal(0, 0.1, size=(M, K))\n",
    "    V = np.random.normal(0, 0.1, size=(N, K))\n",
    "    F = np.random.normal(0, 0.1, size=(num_features, K))\n",
    "\n",
    "    for iteration in range(max_iters):\n",
    "        for m in range(M):\n",
    "            start = user_full_ptr[m]\n",
    "            end = user_full_ptr[m+1]\n",
    "            user_slice = user_full[start:end]\n",
    "            item_ids = user_slice['item']\n",
    "            ratings = user_slice['rating']\n",
    "\n",
    "            if len(item_ids) == 0:\n",
    "                continue\n",
    "            \n",
    "            v = V[item_ids]\n",
    "            i_bias = item_biases[item_ids]\n",
    "\n",
    "            bias_residuals = ratings - np.dot(U[m], v.T) - i_bias\n",
    "\n",
    "            denom = lam * len(item_ids) + gamma\n",
    "            if denom == 0:\n",
    "                user_biases[m] = 0\n",
    "            else :\n",
    "                user_biases[m] = lam * np.sum(bias_residuals) / denom\n",
    "\n",
    "            vector_residual = ratings - i_bias - user_biases[m]\n",
    "            residual = lam * np.sum(vector_residual[:, np.newaxis] * v, axis=0)\n",
    "            A = lam * np.dot(v.T, v) + tau * np.eye(K)\n",
    "            U[m] = np.linalg.solve(A, residual)\n",
    "\n",
    "        for n in range(N):\n",
    "            start = item_full_ptr[n]\n",
    "            end = item_full_ptr[n+1]\n",
    "            item_slice = item_full[start:end]\n",
    "            user_ids = item_slice['user']\n",
    "            ratings = item_slice['rating']\n",
    "\n",
    "            if len(user_ids) == 0:\n",
    "                continue\n",
    "\n",
    "            u = U[user_ids]\n",
    "            u_bias = user_biases[user_ids]\n",
    "\n",
    "            bias_residuals = ratings - np.dot(u, V[n]) - u_bias\n",
    "            denom = lam * len(user_ids) + gamma\n",
    "            if denom == 0:\n",
    "                item_biases[n] = 0 \n",
    "            else:\n",
    "                item_biases[n] = lam * np.sum(bias_residuals) / denom\n",
    "\n",
    "            vector_residual = ratings - u_bias - item_biases[n]\n",
    "            residual = lam * np.sum(vector_residual[:, np.newaxis] * u, axis=0)\n",
    "            A = lam * np.dot(u.T, u) + tau * np.eye(K)\n",
    "\n",
    "            f_idx = np.flatnonzero(item_features[n])  \n",
    "            Fn = len(f_idx)\n",
    "            if Fn > 0:\n",
    "                feature_mean = F[f_idx].sum(axis=0) / np.sqrt(Fn)\n",
    "            else:\n",
    "                feature_mean = np.zeros(K)\n",
    "            V[n] = np.linalg.solve(A, residual + tau * feature_mean)\n",
    "\n",
    "        for l in range(num_features):\n",
    "            items_with_l = np.flatnonzero(item_features[:, l])\n",
    "            if items_with_l.size == 0:\n",
    "                continue\n",
    "\n",
    "            num = np.zeros(K)\n",
    "            denom = 1\n",
    "            for n in items_with_l:\n",
    "                f_idx = np.flatnonzero(item_features[n])\n",
    "                Fn = len(f_idx)\n",
    "                if Fn == 0:\n",
    "                    continue\n",
    "\n",
    "                sum_except_l = F[f_idx].sum(axis=0) - F[l]\n",
    "                num += (V[n] - (sum_except_l / np.sqrt(Fn))) / np.sqrt(Fn)   \n",
    "                denom += 1 / Fn\n",
    "            F[l] = num / denom\n",
    "\n",
    "\n",
    "        err =  0\n",
    "        count = 0\n",
    "\n",
    "        for m in range(M):\n",
    "            u_bias = user_biases[m]\n",
    "            u_vec = U[m]\n",
    "            start = user_full_ptr[m]\n",
    "            end = user_full_ptr[m+1]\n",
    "            user_slice = user_full[start:end]\n",
    "            if len(user_slice) > 0:\n",
    "                item_ids = user_slice['item']\n",
    "                ratings = user_slice['rating']\n",
    "                preds = np.dot(u_vec, V[item_ids].T) + u_bias + item_biases[item_ids]\n",
    "                err += np.sum((ratings - preds) ** 2)\n",
    "                count += len(ratings)\n",
    "        \n",
    "        feature_prior = 0\n",
    "        for n in range(N):\n",
    "            f_idx = np.flatnonzero(item_features[n])\n",
    "            Fn = len(f_idx)\n",
    "            if Fn > 0:\n",
    "                feature_mean = F[f_idx].sum(axis=0) / np.sqrt(Fn)\n",
    "            else:\n",
    "                feature_mean = np.zeros(K)\n",
    "            feature_prior += np.sum((V[n] - feature_mean) ** 2)\n",
    "\n",
    "        loss = 0.5 * lam  * err + 0.5 * gamma * (np.sum(user_biases ** 2) + np.sum(item_biases ** 2)) + 0.5 * tau * (np.sum(U**2) +feature_prior + np.sum(F**2)) \n",
    "        loss_history.append(loss)\n",
    "\n",
    "        RMSE = np.sqrt(err / count)\n",
    "        RMSE_history.append(RMSE)\n",
    "\n",
    "\n",
    "        if log:\n",
    "            print(f\"Iter {iteration+1:3d}  | \" f\" Loss: {loss:.4f} | \" f\" RMSE: {RMSE:.4f}\")\n",
    "\n",
    "        if iteration > 4 and abs(RMSE_history[-1] - RMSE_history[-5]) < tol:\n",
    "            if log:\n",
    "                print(f\"Early stopping at iteration {iteration + 1}\")\n",
    "            break\n",
    "\n",
    "    history = {\"loss\": loss_history, \"RMSE\": RMSE_history}\n",
    "\n",
    "    return user_biases, item_biases, U, V , F, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023835c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_biases, i_biases, U , V, F, history =  ALS_Features_Full(user_full, item_full, user_full_ptr, item_full_ptr, M, N, K, item_features,\n",
    "                                                    lam=lam, gamma=gamma, tau = tau, max_iters=50, tol=1e-4, log=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326f6907",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "V_2d = pca.fit_transform(V)\n",
    "\n",
    "Horror = [39446, 48877, 55577]\n",
    "SciFi = [260, 1196, 1210]\n",
    "RomCom = [3, 7, 64]\n",
    "Children = [78499, 49274, 62999]\n",
    "\n",
    "groups = { \"Horror\": Horror, \"Sci-Fi\": SciFi, \"RomCom\": RomCom, \"Children\": Children}\n",
    "colors = {\"Horror\": \"tab:orange\", \"Sci-Fi\": \"tab:blue\", \"RomCom\": \"tab:red\",\"Children\": \"tab:purple\"}\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(V_2d[:, 0], V_2d[:, 1], s=3, c=\"#00cc44\", alpha=0.2, label=\"All movies\")\n",
    "\n",
    "for group_name, movie_ids in groups.items():\n",
    "    indices = [idx_i.index(mid) for mid in movie_ids if mid in idx_i]\n",
    "    plt.scatter(V_2d[indices, 0], V_2d[indices, 1], s=50, c=colors[group_name], label=group_name,\n",
    "                edgecolors=\"black\", linewidths=0.5)\n",
    "    for idx in indices:\n",
    "        movie_id = idx_i[idx]\n",
    "        title = movie_titles.get(movie_id, f\"movieId={movie_id}\")\n",
    "        plt.text(V_2d[idx, 0] + 0.02, V_2d[idx, 1] + 0.02, title, fontsize=8)\n",
    "\n",
    "\n",
    "plt.axhline(0, lw=0.8, color=\"gray\", alpha=0.5)\n",
    "plt.axvline(0, lw=0.8, color=\"gray\", alpha=0.5)\n",
    "plt.title(\"2D Embedding of Movies\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"2D embeddings with features full set.pdf\", format=\"pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793ed019",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, random_state=0)\n",
    "V_2d = pca.fit_transform(V)         \n",
    "F_2d = pca.transform(F)              \n",
    "\n",
    "index_to_genre = {i: g for g, i in genre_to_index.items()}\n",
    "\n",
    "genres_to_annotate = [\"Action\", \"Comedy\", \"Drama\", \"Sci-Fi\", \"Romance\"]\n",
    "annot_idx = [genre_to_index[g] for g in genres_to_annotate if g in genre_to_index]\n",
    "\n",
    "plt.figure(figsize=(11, 8.5))\n",
    "\n",
    "plt.scatter(V_2d[:, 0], V_2d[:, 1], s=3, c=\"#00cc44\", alpha=0.18, label=\"All movies\")\n",
    "\n",
    "plt.scatter(F_2d[:, 0], F_2d[:, 1], s=120, marker=\"*\", edgecolors=\"black\",\n",
    "            linewidths=0.6, alpha=0.95, c=\"tab:purple\", label=\"Feature vectors (genres)\")\n",
    "\n",
    "for gi in range(F_2d.shape[0]):\n",
    "    x, y = F_2d[gi]\n",
    "    plt.text(x + 0.02, y + 0.02, index_to_genre[gi], fontsize=8)\n",
    "\n",
    "\n",
    "plt.axhline(0, lw=0.8, color=\"gray\", alpha=0.45)\n",
    "plt.axvline(0, lw=0.8, color=\"gray\", alpha=0.45)\n",
    "plt.title(\"2D PCA of Movie Embeddings with Features\", fontsize=12)\n",
    "plt.legend(loc=\"best\", fontsize=9, frameon=True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"FEATURE EMBEDDINGS full set.pdf\", format=\"pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43c4187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cars 45517  LOTR 2116, 4993   Cold start movie 52696\n",
    "\n",
    "#Horror = [39446, 48877, 55577]\n",
    "#SciFi = [260, 1196, 1210]\n",
    "#RomCom = [3, 7, 64]\n",
    "#Superhero = [33794, 59315, 77561]\n",
    "\n",
    "movieID = 79363\n",
    "\n",
    "r_dummy = 5\n",
    "u_dummy = np.zeros(K)\n",
    "b_dummy = 0\n",
    "\n",
    "is_cold_start = False\n",
    "\n",
    "if movieID in i_idx:\n",
    "    item_idx = i_idx[movieID]\n",
    "    v = V[item_idx]\n",
    "    i_bias = i_biases[item_idx]\n",
    "else:\n",
    "    is_cold_start = True\n",
    "    genres = movie_id_to_genres.get(movieID, [])\n",
    "    f_idx = [genre_to_index[g] for g in genres if g in genre_to_index]\n",
    "\n",
    "    if not f_idx:\n",
    "        v = np.random.normal(0, 0.01, size=K)\n",
    "        i_bias = 0\n",
    "    else:\n",
    "        Fn = len(f_idx)\n",
    "        v = F[f_idx].sum(axis=0) / np.sqrt(Fn)  \n",
    "        i_bias = np.mean(i_biases)                    \n",
    "\n",
    "for i in range(20):\n",
    "    b_dummy = lam * (r_dummy - np.dot(u_dummy, v) - i_bias) / (lam + gamma)\n",
    "    u_dummy = ((r_dummy - b_dummy - i_bias) * v) / (np.dot(v, v) + tau)\n",
    "\n",
    "scores = V @ u_dummy + 0.05 * i_biases\n",
    "\n",
    "min_ratings = 100\n",
    "scores = np.where(np.array(num_ratings) >= min_ratings, scores, -np.inf)\n",
    "\n",
    "top_items = np.argsort(scores)[::-1]\n",
    "top_n = 5\n",
    "shown = 0\n",
    "\n",
    "print(f\"\\nTop {top_n} recommendations for a dummy \" f\"'{movie_titles.get(movieID, f'movieId={movieID}')}' fan \")\n",
    "\n",
    "for idx in top_items:\n",
    "    movie_id = idx_i[idx]\n",
    "    if movie_id == movieID:\n",
    "        continue\n",
    "    title = movie_titles.get(movie_id, f\"movieId={movie_id}\")\n",
    "    genres = movie_id_to_genres.get(movie_id, [\"(no genres listed)\"])\n",
    "\n",
    "    print(f\"{shown+1:2d}. {title} â€” Genres: {', '.join(genres)}\")\n",
    "    shown += 1\n",
    "    if shown >= top_n:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b8f31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_norms = np.linalg.norm(V, axis=1)\n",
    "\n",
    "min_ratings = 100\n",
    "valid_indices = [i for i, n in enumerate(num_ratings) if n >= min_ratings]\n",
    "\n",
    "polarizing_indices = sorted(valid_indices, key=lambda i: movie_norms[i], reverse=True)\n",
    "least_polarizing_indices = sorted(valid_indices, key=lambda i: movie_norms[i])\n",
    "\n",
    "top_n = 10\n",
    "\n",
    "print(f\"\\nTop {top_n} MOST polarizing movies:\")\n",
    "for idx in polarizing_indices[:top_n]:\n",
    "    movie_id = idx_i[idx]\n",
    "    print(f\"- {movie_titles.get(movie_id, f'movieId={movie_id}')} \" f\"(norm={movie_norms[idx]:.4f}, ratings={num_ratings[idx]})\")\n",
    "\n",
    "print(f\"\\nTop {top_n} LEAST polarizing movies:\")\n",
    "for idx in least_polarizing_indices[:top_n]:\n",
    "    movie_id = idx_i[idx]\n",
    "    print(f\"- {movie_titles.get(movie_id, f'movieId={movie_id}')} \" f\"(norm={movie_norms[idx]:.4f}, ratings={num_ratings[idx]})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca6d8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    lam = trial.suggest_float(\"lam\", 0.02, 0.2, log=True)\n",
    "    gamma = trial.suggest_float(\"gamma\", 0.05, 0.3, log=True)\n",
    "\n",
    "    user_biases, item_biases, history = ALS_Bias(\n",
    "        user_train=user_train,\n",
    "        item_train=item_train,\n",
    "        user_test=user_test,\n",
    "        item_test=item_test,\n",
    "        user_train_ptr=user_train_ptr,\n",
    "        item_train_ptr=item_train_ptr,\n",
    "        user_test_ptr=user_test_ptr,\n",
    "        item_test_ptr=item_test_ptr,\n",
    "        M=M,\n",
    "        N=N,\n",
    "        lam=lam,\n",
    "        gamma=gamma,\n",
    "        max_iters=max_iters,\n",
    "        tol=tol,\n",
    "        log=False      \n",
    "    )\n",
    "\n",
    "    test_rmse = history[\"test_RMSE\"][-1]\n",
    "\n",
    "    for i, rm in enumerate(history[\"test_RMSE\"], start=1):\n",
    "        trial.report(rm, i)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    return test_rmse\n",
    "\n",
    "sampler = TPESampler(seed=0, multivariate=True)\n",
    "pruner = MedianPruner(n_startup_trials=10, n_warmup_steps=5)\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\", sampler=sampler, pruner=pruner)\n",
    "study.optimize(objective, n_trials=30, show_progress_bar=True)\n",
    "\n",
    "print(\"Best value (test RMSE):\", study.best_value)\n",
    "print(\"Best params:\", study.best_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e54dea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    K = trial.suggest_categorical(\"K\", [5, 8,10, 12, 15 ])\n",
    "    lam = trial.suggest_float(\"lam\", 0.02, 0.2, log=True)\n",
    "    gamma = trial.suggest_float(\"gamma\", 0.05, 0.3, log=True)\n",
    "    tau = trial.suggest_float(\"tau\", 0.02, 0.2, log=True)\n",
    "\n",
    "\n",
    "\n",
    "    user_biases, item_biases, U, V, history = ALS_Embedding(\n",
    "        user_train=user_train,\n",
    "        item_train=item_train,\n",
    "        user_test=user_test,\n",
    "        item_test=item_test,\n",
    "        user_train_ptr=user_train_ptr,\n",
    "        item_train_ptr=item_train_ptr,\n",
    "        user_test_ptr=user_test_ptr,\n",
    "        item_test_ptr=item_test_ptr,\n",
    "        M=M,\n",
    "        N=N,\n",
    "        K=K,\n",
    "        lam=lam,\n",
    "        gamma=gamma,\n",
    "        tau=tau,\n",
    "        max_iters=max_iters,\n",
    "        tol=tol,\n",
    "        log=False)\n",
    "\n",
    "    for step, rm in enumerate(history[\"test_RMSE\"], start=1):\n",
    "        trial.report(rm, step)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "\n",
    "    return history[\"test_RMSE\"][-1]\n",
    "\n",
    "\n",
    "sampler = TPESampler(seed=0, multivariate=True)\n",
    "pruner = MedianPruner(n_startup_trials=10, n_warmup_steps=5)\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\", sampler=sampler, pruner=pruner)\n",
    "study.optimize(objective, n_trials=30, show_progress_bar=True)\n",
    "\n",
    "print(\"Best value (test RMSE):\", study.best_value)\n",
    "print(\"Best params:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1142174e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    K = trial.suggest_categorical(\"K\", [8 ,10, 12])\n",
    "    lam = trial.suggest_float(\"lam\", 0.01, 0.1, log=True)\n",
    "    gamma = trial.suggest_float(\"gamma\", 0.05, 0.15, log=True)\n",
    "    tau = trial.suggest_float(\"tau\", 0.05, 0.15, log=True)\n",
    "\n",
    "    user_biases, item_biases, U, V, F, history = ALS_Features(\n",
    "        user_train=user_train,\n",
    "        item_train=item_train,\n",
    "        user_test=user_test,\n",
    "        item_test=item_test,\n",
    "        user_train_ptr=user_train_ptr,\n",
    "        item_train_ptr=item_train_ptr,\n",
    "        user_test_ptr=user_test_ptr,\n",
    "        item_test_ptr=item_test_ptr,\n",
    "        M=M,\n",
    "        N=N,\n",
    "        K=K,\n",
    "        item_features=item_features,\n",
    "        lam=lam,\n",
    "        gamma=gamma,\n",
    "        tau=tau,\n",
    "        max_iters= 50,\n",
    "        tol=tol,\n",
    "        log=False )\n",
    "\n",
    "    for step, rm in enumerate(history[\"test_RMSE\"], start=1):\n",
    "        trial.report(rm, step)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "   \n",
    "    return history[\"test_RMSE\"][-1]\n",
    "\n",
    "\n",
    "sampler = TPESampler(seed=0, multivariate=True)\n",
    "pruner = MedianPruner(n_startup_trials=10, n_warmup_steps=5)\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\", sampler=sampler, pruner=pruner)\n",
    "study.optimize(objective, n_trials=15, show_progress_bar=True)\n",
    "\n",
    "print(\"Best value (test RMSE):\", study.best_value)\n",
    "print(\"Best params:\", study.best_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b487ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_implicit(data_by_user, data_by_item, threshold=4.0):\n",
    "    M = len(data_by_user)\n",
    "    positive_items_by_user = [set() for _ in range(M)]\n",
    "    for m in range(M):\n",
    "        for n, r in data_by_user[m]:\n",
    "            if r >= threshold:\n",
    "                positive_items_by_user[m].add(n)\n",
    "    return positive_items_by_user\n",
    "\n",
    "def sample(pos_items_by_user, N, batch_users, max_resample=20, device=\"cpu\"):\n",
    "    u = torch.tensor(batch_users, dtype=torch.long, device=device)\n",
    "    i = torch.tensor([np.random.choice(list(pos_items_by_user[user])) for user in batch_users],dtype=torch.long, device=device)\n",
    "\n",
    "    j_list = np.random.randint(0, N, size=len(batch_users)).tolist()\n",
    "    for k, user in enumerate(batch_users):\n",
    "        tries = 0\n",
    "        while j_list[k] in pos_items_by_user[user] and tries < max_resample:\n",
    "            j_list[k] = np.random.randint(0, N)\n",
    "            tries += 1\n",
    "    j = torch.tensor(j_list, dtype=torch.long, device=device)\n",
    "\n",
    "    return u, i, j\n",
    "\n",
    "class BPR(nn.Module):\n",
    "    def __init__(self, M, N, K):\n",
    "        super().__init__()\n",
    "        self.user_emb = nn.Embedding(M, K)\n",
    "        self.item_emb = nn.Embedding(N, K)\n",
    "        nn.init.normal_(self.user_emb.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_emb.weight, std=0.01)\n",
    "\n",
    "    def forward(self, u, i, j):\n",
    "        u_vec = self.user_emb(u)\n",
    "        i_vec = self.item_emb(i)\n",
    "        j_vec = self.item_emb(j)\n",
    "        r_ui = torch.sum(u_vec * i_vec, dim=1)\n",
    "        r_uj = torch.sum(u_vec * j_vec, dim=1)\n",
    "        return r_ui - r_uj \n",
    "\n",
    "def train_bpr_torch(model, pos_items_by_user,n_epochs=10, batch_size=4096, lr=0.01, reg=0.01,seed=0, device=\"cpu\"):\n",
    "    torch.manual_seed(seed)\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    M = model.user_emb.num_embeddings\n",
    "    N = model.item_emb.num_embeddings\n",
    "\n",
    "    train_users = np.array([u for u in range(M) if len(pos_items_by_user[u]) > 0], dtype=np.int64)\n",
    "    steps_per_epoch = max(1, len(train_users) // batch_size)\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        rng.shuffle(train_users)\n",
    "\n",
    "        for step in range(steps_per_epoch):\n",
    "            batch_users = train_users[step*batch_size:(step+1)*batch_size]\n",
    "            if len(batch_users) == 0:\n",
    "                continue\n",
    "\n",
    "            u, i, j = sample(pos_items_by_user, N, batch_users, device=device)\n",
    "            r_uij = model(u, i, j)\n",
    "\n",
    "            loss = -F_torch.logsigmoid(r_uij).mean()\n",
    "            reg_loss = sum(torch.norm(p, p=2).pow(2) for p in model.parameters())\n",
    "            loss += reg * reg_loss / batch_size\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch}/{n_epochs}, Loss={loss.item():.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def Evaluate_at_k(model, test_pos_by_user, train_pos_by_user=None, K=10):\n",
    "    U = model.user_emb.weight.detach().cpu().numpy()\n",
    "    V = model.item_emb.weight.detach().cpu().numpy()\n",
    "\n",
    "    precisions, recalls, ndcgs = [], [], []\n",
    "\n",
    "    for u, true_items in enumerate(test_pos_by_user):\n",
    "        if not true_items:\n",
    "            continue\n",
    "        exclude = train_pos_by_user[u] if train_pos_by_user else set()\n",
    "\n",
    "        scores = U[u] @ V.T\n",
    "        if exclude:\n",
    "            scores[list(exclude)] = -np.inf\n",
    "        topk = np.argsort(-scores)[:K]\n",
    "\n",
    "        hits = [1 if i in true_items else 0 for i in topk]\n",
    "        n_hit = sum(hits)\n",
    "        precisions.append(n_hit / K)\n",
    "        recalls.append(n_hit / len(true_items))\n",
    "\n",
    "        dcg = 0\n",
    "        for idx, h in enumerate(hits):\n",
    "            dcg += h / np.log2(idx + 2)\n",
    "\n",
    "        idcg = 0\n",
    "        for i in range(min(len(true_items), K)):\n",
    "            idcg += 1 / np.log2(i + 2)\n",
    "\n",
    "        if idcg > 0:\n",
    "            ndcgs.append(dcg / idcg)\n",
    "        else:\n",
    "            ndcgs.append(0)\n",
    "\n",
    "    return np.mean(precisions), np.mean(recalls), np.mean(ndcgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b63926",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = len(data_by_user_train)   \n",
    "N = len(data_by_item_train)   \n",
    "K = 50                        \n",
    "\n",
    "pos_items_by_user_train = build_implicit(data_by_user_train, data_by_item_train, threshold=4.0)\n",
    "pos_items_by_user_test  = build_implicit(data_by_user_test,  data_by_item_test,  threshold=4.0)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = BPR(M, N, K)\n",
    "trained_model = train_bpr_torch(model, pos_items_by_user_train,n_epochs=20, batch_size=4096,lr=0.004, reg=0.002, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e984e007",
   "metadata": {},
   "outputs": [],
   "source": [
    "P, R, NDCG = Evaluate_at_k(trained_model, pos_items_by_user_test, pos_items_by_user_train, K=50)\n",
    "print(f\"Precision={P:.4f}, Recall={R:.4f}, NDCG={NDCG:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10130d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    K          = trial.suggest_categorical(\"K\", [32, 50, 100, 200])\n",
    "    lr         = trial.suggest_float(\"lr\", 1e-4, 5e-2, log=True)\n",
    "    reg        = trial.suggest_float(\"reg\", 1e-5, 1e-1, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [1024, 2048, 4096, 8192])\n",
    "    n_epochs   = trial.suggest_int(\"n_epochs\", 10, 30)\n",
    "    seed       = trial.suggest_int(\"seed\", 0, 10)\n",
    "\n",
    "\n",
    "    M = len(data_by_user_train)\n",
    "    N = len(data_by_item_train)\n",
    "    model = BPR(M, N, K)\n",
    "\n",
    "   \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    trained = train_bpr_torch(\n",
    "        model,\n",
    "        pos_items_by_user_train,\n",
    "        n_epochs=n_epochs,\n",
    "        batch_size=batch_size,\n",
    "        lr=lr,\n",
    "        reg=reg,\n",
    "        seed=seed,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "\n",
    "    P, R, NDCG = Evaluate_at_k(\n",
    "        trained,\n",
    "        pos_items_by_user_test,\n",
    "        pos_items_by_user_train,\n",
    "        K=10\n",
    "    )\n",
    "\n",
    "\n",
    "    trial.set_user_attr(\"Precision@10\", float(P))\n",
    "    trial.set_user_attr(\"Recall@10\",    float(R))\n",
    "\n",
    "\n",
    "    return float(NDCG)\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=\"BPR_implicit_tuning\")\n",
    "study.optimize(objective, n_trials=30, show_progress_bar=True)\n",
    "\n",
    "print(\"\\nBest trial:\")\n",
    "print(f\"  Value (NDCG@10): {study.best_value:.6f}\")\n",
    "print(\"  Params:\")\n",
    "for k, v in study.best_params.items():\n",
    "    print(f\"    {k}: {v}\")\n",
    "\n",
    "\n",
    "best = study.best_params\n",
    "K_best          = best[\"K\"]\n",
    "lr_best         = best[\"lr\"]\n",
    "reg_best        = best[\"reg\"]\n",
    "batch_best      = best[\"batch_size\"]\n",
    "epochs_best     = best[\"n_epochs\"]\n",
    "seed_best       = best[\"seed\"]\n",
    "\n",
    "M = len(data_by_user_train)\n",
    "N = len(data_by_item_train)\n",
    "final_model = BPR(M, N, K_best)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "final_model = train_bpr_torch(\n",
    "    final_model,\n",
    "    pos_items_by_user_train,\n",
    "    n_epochs=epochs_best,\n",
    "    batch_size=batch_best,\n",
    "    lr=lr_best,\n",
    "    reg=reg_best,\n",
    "    seed=seed_best,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "P, R, NDCG = Evaluate_at_k(final_model, pos_items_by_user_test, pos_items_by_user_train, K=10)\n",
    "print(f\"\\nFinal (best) â€” Precision@10={P:.6f}, Recall@10={R:.6f}, NDCG@10={NDCG:.6f}\")\n",
    "print(\"Best hyperparameters:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ea25c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createreco(userid, versionid, user_ratings):\n",
    "    u_dummy = np.zeros(K)\n",
    "    b_dummy = 0\n",
    "\n",
    "    min_ratings = 100\n",
    "\n",
    "    for (item_idx, r_dummy) in user_ratings:\n",
    "        movieID = idx_i[item_idx]\n",
    "\n",
    "        if movieID in i_idx:\n",
    "            v = V[item_idx]\n",
    "            i_bias = i_biases[item_idx]\n",
    "        else:\n",
    "            genres = movie_id_to_genres.get(movieID, [])\n",
    "            f_idx = [genre_to_index[g] for g in genres if g in genre_to_index]\n",
    "            if not f_idx:\n",
    "                v = np.random.normal(0, 0.01, size=K)\n",
    "                i_bias = 0\n",
    "            else:\n",
    "                Fn = len(f_idx)\n",
    "                v = F[f_idx].sum(axis=0) / np.sqrt(Fn)\n",
    "                i_bias = np.mean(i_biases)\n",
    "\n",
    "        for _ in range(20):\n",
    "            b_dummy = lam * (r_dummy - np.dot(u_dummy, v) - i_bias) / (lam + gamma)\n",
    "            u_dummy = ((r_dummy - b_dummy - i_bias) * v) / (np.dot(v, v) + tau)\n",
    "\n",
    "    bias_weight = 0.05 if versionid == \"A\" else 0.2\n",
    "    scores = V @ u_dummy + bias_weight * i_biases\n",
    "    scores = np.where(np.array(num_ratings) >= min_ratings, scores, -np.inf)\n",
    "\n",
    "    top_n = 5\n",
    "    top_items = np.argsort(scores)[::-1]\n",
    "    recommendations = []\n",
    "    shown = 0\n",
    "\n",
    "    rated_items = set([i for i, _ in user_ratings]) \n",
    "\n",
    "    for idx in top_items:\n",
    "        if idx in rated_items:\n",
    "            continue\n",
    "        movie_id = idx_i[idx]\n",
    "        title = movie_titles.get(movie_id, f\"movieId={movie_id}\")\n",
    "        recommendations.append((movie_id, title))\n",
    "        shown += 1\n",
    "        if shown >= top_n:\n",
    "            break\n",
    "\n",
    "    return recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b561ee59",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "num_dummy_users = 100\n",
    "movies_per_user = 50\n",
    "num_items = len(idx_i)\n",
    "\n",
    "high_rating = np.array([4.0, 4.5, 5.0])  \n",
    "all_ratings = np.arange(1.0, 5.5, 0.5) \n",
    "\n",
    "genre_to_movies = {}\n",
    "for movie_id, genres in movie_id_to_genres.items():\n",
    "    for g in genres:\n",
    "        if g not in genre_to_movies:\n",
    "            genre_to_movies[g] = []\n",
    "        if movie_id in i_idx:\n",
    "            genre_to_movies[g].append(i_idx[movie_id]) \n",
    "\n",
    "dummy_dataset = []\n",
    "preferred_movies_by_user = []\n",
    "preferred_genres = []\n",
    "\n",
    "for u in range(num_dummy_users):\n",
    "    preferred_genre = rng.choice(all_genres)\n",
    "    preferred_genres.append(preferred_genre)\n",
    "    preferred_movies = rng.choice(genre_to_movies[preferred_genre], size=movies_per_user, replace=False)\n",
    "    preferred_ratings = [(idx, rng.choice(high_rating)) for idx in preferred_movies]\n",
    "    ratings = preferred_ratings\n",
    "    rng.shuffle(ratings)\n",
    "    dummy_dataset.append(ratings)\n",
    "    preferred_movies_by_user.append(preferred_movies)\n",
    "\n",
    "dummy_user_ratings, dummy_user_ptr = vectorize(dummy_dataset, \"item\")\n",
    "\n",
    "version_ids = rng.choice([\"A\", \"B\"], size=num_dummy_users)\n",
    "group_A_users = np.where(version_ids == \"A\")[0]\n",
    "group_B_users = np.where(version_ids == \"B\")[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3282a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = []\n",
    "genre_hits_A, genre_hits_B = 0, 0\n",
    "k = 5  \n",
    "\n",
    "for u in range(num_dummy_users):\n",
    "    version = version_ids[u]\n",
    "    start = dummy_user_ptr[u]\n",
    "    end = dummy_user_ptr[u + 1]\n",
    "    all_user_ratings = dummy_user_ratings[start:end]  \n",
    "\n",
    "    train_ratings = list(all_user_ratings)\n",
    "    recs = createreco(userid=u, versionid=version, user_ratings=train_ratings)\n",
    "    for rank, (movie_id, title) in enumerate(recs, start=1):\n",
    "        logs.append({\"user_id\": u, \"version\": version,  \"Rank\": rank, \"movie_id\": movie_id, \"title\": title })\n",
    "\n",
    "\n",
    "    preferred_genre = preferred_genres[u]\n",
    "    rec_genres = [movie_id_to_genres.get(movie_id, []) for movie_id, _ in recs]\n",
    "    hits = sum(1 for genres in rec_genres if preferred_genre in genres)\n",
    "\n",
    "\n",
    "    if version == \"A\":\n",
    "        genre_hits_A += hits\n",
    "    else:\n",
    "        genre_hits_B += hits\n",
    "\n",
    "\n",
    "logs_df = pd.DataFrame(logs)\n",
    "logs_df.to_csv(\"AB Testing recommendations.csv\", index=False)\n",
    "print(\"Saved recommendations for 100 dummy users to AB Testing recommendations.csv\")\n",
    "\n",
    "\n",
    "genre_recall_A = genre_hits_A / (len(group_A_users) * k)\n",
    "genre_recall_B = genre_hits_B / (len(group_B_users) * k)\n",
    "\n",
    "print(f\"Version A â€” GenreRecall@{k}: {genre_recall_A:.4f}\")\n",
    "print(f\"Version B â€” GenreRecall@{k}: {genre_recall_B:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678583b9",
   "metadata": {},
   "source": [
    "ChatGPT was used to help with:\n",
    "    - Graph plotting\n",
    "    - Logging of results\n",
    "    - Pytorch implmentation\n",
    "    - Optuna set up\n",
    "    - Implementation of implicit system"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "custom_cell_magics": "kql"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
